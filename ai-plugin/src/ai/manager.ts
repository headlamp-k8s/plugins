export type Prompt = {
  role: string;
  content: string;
  toolCalls?: any[];
  toolCallId?: string;
  name?: string;
  error?: boolean;
  contentFilterError?: boolean;
  alreadyDisplayed?: boolean;
  isDisplayOnly?: boolean; // Mark messages that shouldn't be sent to LLM
};

export default abstract class AIManager {
  history: Prompt[] = [];
  currentContext: string = '';

  setContext(contextDescription: string) {
    this.currentContext = contextDescription;
  }

  addContextualInfo(info: string) {
    if (this.currentContext) {
      this.currentContext += '\n' + info;
    } else {
      this.currentContext = info;
    }
  }

  reset() {
    this.history = [];
    this.currentContext = '';
  }

  // Abstract method that must be implemented
  abstract userSend(message: string): Promise<Prompt>;

  // Changed from protected to public to allow external calling
  abstract processToolResponses(): Promise<Prompt>;

  // Abstract method to abort current request
  abstract abort(): void;

  // Define configureTools method for tool configuration - made generic to support different contexts
  configureTools?(tools: any[], context: any): void;

  getPromptSuggestions(): string[] {
    // Return empty array as suggestions are now dynamically generated by the LLM
    // and parsed from responses in the modal component
    return [];
  }
}
